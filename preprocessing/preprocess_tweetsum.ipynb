{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocess_tweetsum.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN7rWovm/W/nieDeZQ+y+nZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"t85z8yi0Cnso","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638857463671,"user_tz":300,"elapsed":21992,"user":{"displayName":"Sara Haman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghrkm05RoKPUOEy_8fBWgAXP-_ELu51T_jQRs3xPg=s64","userId":"16382698357113713759"}},"outputId":"517087af-50f1-4ec5-8b3d-eb2928dcfb37"},"source":["# ------------------------- # \n","#        SET - UP           # \n","# ------------------------- # \n","\n","# ---- Requirements ----- # \n","\n","#!pip install datasets\n","#!pip install sentencepiece\n","#!pip install transformers\n","#!pip install jsonlines\n","\n","import csv\n","import datasets\n","from google.colab import drive\n","import huggingface_hub\n","import jsonlines\n","import json\n","import pandas as pd\n","import re\n","import sys\n","\n","# ----- Check if GPU is connected ----- # \n","gpu_info = !nvidia-smi -L\n","gpu_info = \"\\n\".join(gpu_info)\n","if gpu_info.find(\"failed\") >= 0:\n","    print(\"Not connected to a GPU\")\n","else:\n","    print(gpu_info)\n","\n","# ----- Mounting Google Drive ----- # \n","\n","drive.mount('/content/drive')\n","sys.path.append('/content/drive/MyDrive/CIS6930_final')\n","\n","# ----- Importing TweetSum processing module ----- #\n","from tweet_sum_processor import TweetSumProcessor\n","\n","# ----------------------------------------------------------------------\n","\n","# ------------------------- # \n","#  PRE-PROCESSING FUNCTIONS # \n","# ------------------------- # \n","\n","\n","def get_inputs(json_format):\n","  '''\n","  ---------------------\n","  Input: Dictionary containing the metadata for one tweet conversation\n","  Output: Concatenated string containing the content of one conversation. \n","  \n","  Notes: \n","          Special characters inserted for links and transitions between speaker. \n","          Anonymized usernames are removed, as they do not add value to the text, \n","          as they are usually just located at the beginning of the tweet by default \n","          (feature of threads). Whereas usernames containing the name of the business \n","          are retained for contextual purpose. \n","  ---------------------\n","  '''\n","  dialogue = json_format['dialog']['turns']\n","  full_text = []\n","  for i in dialogue:\n","    string = ' '.join(i['sentences'])\n","    full_text.append(string + \" <BR>\")\n","  conversation = ' '.join(full_text)\n","  by_word = conversation.split(' ')\n","  for i in range(0, len(by_word)):\n","    if \"https\" in by_word[i]:\n","      by_word[i] = \"<LINK>\"\n","    if \"@\" in by_word[i]:\n","      if by_word[i][1:].isnumeric():\n","        by_word[i] = ''\n","  text = ' '.join(by_word)\n","  text = re.sub(r'[^a-zA-Z0-9,!.?<> ]', '', text)\n","  text = re.sub(r'(\\W)(?=\\1)', '', text)\n","  return text\n","  \n","def get_summary(json_format): \n","  '''\n","  ---------------------\n","  Input: Dictionary containing the metadata for one tweet conversation\n","  Output: The text of a single human-generated summary for that one tweet\n","  ---------------------\n","  '''\n","  temp = json_format['summaries']['abstractive_summaries'][0]\n","  summary = ' '.join(temp)\n","  return summary\n","\n","def prepare_data(file_name, processor):\n","  '''\n","  Processing the TweetSum dataset so that it can be read as a HuggingFace dataset. \n","  ---------------------\n","  Input: Path to a dataset file and the TweetSum processor\n","  Output: The inputs and summaries for the given data\n","  ---------------------\n","  '''\n","  inputs = []\n","  summaries = []\n","  with open('/content/drive/MyDrive/CIS6930_final/' + file_name) as f:\n","    dialog_with_summaries = processor.get_dialog_with_summaries(f.readlines())\n","    for dialog_with_summary in dialog_with_summaries:\n","      try:\n","        json_format = json.loads(dialog_with_summary.get_json())\n","        inputs.append(get_inputs(json_format))\n","        summaries.append(get_summary(json_format))\n","      except TypeError:\n","        pass\n","  return inputs, summaries\n","\n","# ----------------------------------------------------------------------\n","\n","# ------------------------- # \n","#           \"MAIN\"          # \n","# ------------------------- # \n","\n","# --- \"Fake\" main function because this is a notebook and not a script :)\n","\n","# ------ Process data \n","\n","processor = TweetSumProcessor('/content/drive/MyDrive/CIS6930_final/kaggle_files/twcs.csv')\n","train_inputs, train_summs = prepare_data('final_train_tweetsum.jsonl', processor)\n","valid_inputs, valid_summs = prepare_data('final_valid_tweetsum.jsonl', processor)\n","test_inputs, test_summs = prepare_data('final_test_tweetsum.jsonl', processor)\n","\n","# ----- Save as CSVs\n","\n","train = pd.DataFrame({\"inputs\": train_inputs, \"summaries\": train_summs})\n","train.to_csv('/content/drive/MyDrive/CIS6930_final/tweetsum_train.csv', index=False)\n","\n","valid = pd.DataFrame({\"inputs\": valid_inputs, \"summaries\": valid_summs})\n","valid.to_csv('/content/drive/MyDrive/CIS6930_final/tweetsum_valid.csv', index=False)\n","\n","test = pd.DataFrame({\"inputs\": test_inputs, \"summaries\": test_summs})\n","test.to_csv('/content/drive/MyDrive/CIS6930_final/tweetsum_test.csv', index=False)\n"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Not connected to a GPU\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]}]}