{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EDA.ipynb","provenance":[],"authorship_tag":"ABX9TyOvovn/Wlbs4yK4iCvmQWmF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"J7k0l4yk3QPj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638858296910,"user_tz":300,"elapsed":161,"user":{"displayName":"Sara Haman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghrkm05RoKPUOEy_8fBWgAXP-_ELu51T_jQRs3xPg=s64","userId":"16382698357113713759"}},"outputId":"77122442-3df9-4fae-dce7-fccf49ed01c5"},"source":["# ------------------------- # \n","#        SET - UP           # \n","# ------------------------- # \n","\n","# ---- Requirements ----- # \n","\n","import sys\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\n","from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers.optimization import Adafactor, AdafactorSchedule\n","import torch\n","import huggingface_hub\n","from datasets import load_dataset, Dataset, load_metric\n","import nltk\n","nltk.download(\"punkt\")\n","import gc\n","import random\n","from torch import nn \n","\n","# ----- Mounting Google Drive ----- # \n","\n","drive.mount('/content/drive')\n","sys.path.append('/content/drive/MyDrive/CIS6930_final')\n","\n","# ----------------------------------------------------------------------\n","\n","# ----- Reading in the Dataset\n","train = pd.read_csv('/content/drive/MyDrive/CIS6930_final/tweetsum_train.csv')\n","valid = pd.read_csv('/content/drive/MyDrive/CIS6930_final/tweetsum_valid.csv')\n","test = pd.read_csv('/content/drive/MyDrive/CIS6930_final/tweetsum_test.csv')\n"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"QF4aSnN_gAlQ"},"source":["# Exploratory Analysis \n","Brief exploratory data analysis to familiarize myself with the data. \n","\n","**Components:**\n","\n","\n","1.   Length\n","2.   Vocabulary size \n","3.   Manual inspection of the data\n","\n"]},{"cell_type":"code","metadata":{"id":"_4LsxggS8swM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638858333050,"user_tz":300,"elapsed":174,"user":{"displayName":"Sara Haman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghrkm05RoKPUOEy_8fBWgAXP-_ELu51T_jQRs3xPg=s64","userId":"16382698357113713759"}},"outputId":"7328d904-614f-446c-861c-5e8024981d9c"},"source":["# ------------------------- # \n","#     VARIABLE LENGTH       # \n","# ------------------------- # \n","\n","# 5 number summary for length of inputs/summaries across datasets \n","\n","for i in [train, valid, test]:\n","  i[\"input_length\"] = i.inputs.str.len()\n","  i[\"summary_length\"] = i.summaries.str.len()\n","  # print(f\"Summary:\\n{i.summary_length.describe()}\\nInputs:\\n{i.input_length.describe()}\")\n","\n","# For all of the data: \n","# --- Approximate how much information loss there will be when the inputs are truncated \n","# --- in the tokenizer. Input length for all models will be held consistent to allow for \n","# --- comparisions of across-model performance \n","\n","temp = pd.concat([train, valid, test])\n","print(f\"Summary:\\n{temp.summary_length.describe()}\\nInputs:\\n{temp.input_length.describe()}\")"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Summary:\n","count    1087.000000\n","mean      193.574057\n","std        63.335348\n","min        72.000000\n","25%       146.000000\n","50%       182.000000\n","75%       226.000000\n","max       515.000000\n","Name: summary_length, dtype: float64\n","Inputs:\n","count    1087.000000\n","mean     1105.671573\n","std       413.260190\n","min       419.000000\n","25%       831.000000\n","50%      1013.000000\n","75%      1270.500000\n","max      3484.000000\n","Name: input_length, dtype: float64\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pzuLxKHrhfDp","executionInfo":{"status":"ok","timestamp":1638858500025,"user_tz":300,"elapsed":190,"user":{"displayName":"Sara Haman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghrkm05RoKPUOEy_8fBWgAXP-_ELu51T_jQRs3xPg=s64","userId":"16382698357113713759"}},"outputId":"1746a939-d122-40b1-fcf5-074204fa546d"},"source":["# ------------------------- # \n","#     VOCABULARY SIZE       # \n","# ------------------------- # \n","\n","summary_vocab_size = len(set(' '.join(temp.summaries.to_list()).split(' ')))\n","input_vocab_size = len(set(' '.join(temp.inputs.to_list()).split(' ')))\n","print(f\"VOCABULARY SIZE\\nSummary vocabulary size: {summary_vocab_size}\\nInput vocabulary size: {input_vocab_size}\")\n"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["VOCABULARY SIZE\n","Summary vocabulary size: 4673\n","Input vocabulary size: 19817\n"]}]},{"cell_type":"code","metadata":{"id":"GFQ7CGERld3k"},"source":["# --------------------- # \n","#    QUALITY CONTROL    #\n","# --------------------- #\n","\n","# The quality of a automatic summary depends on the quality of the ground truth \n","# summaries it is provided. The old adage, \"garbage in, garbage out\", is particularly\n","# relevant here. I randomly sample 20 summaries to inspect the quality of. This is,\n","# of course, being subjective. If this were a project with a greater scrope than a term\n","# project, I would likely have others looking at the summaries and assigning quality scores, \n","# to gauge inter-rater reliability. However, it is just me. \n","\n","# ----------------------------------------------------------------------------------\n","\n","# Thoughts on summaries themselves: \n","# The quality of the summaries is sub-par. There are frequent spelling mistakes, \n","# grammatical errors, and misused words. I am concerned about the quality of the potential\n","# results, from an interpretability standpoint. There are also a surplus of examples that\n","# start with the phrase: \"Customer is complaining\". If the model is able to pick up\n","# on this pattern, it could inflate the scores of metrics which depend on shared n-grams. \n","# Proper capitalization is also inconsistent. \n","\n","# Examples: \n","# \"Customer equires\" -- should be \"inquires\"\n","# \"Customer is disappointed for the delay of the products for two days\" -- improper grammar\n","# \"Customer is complaining  about the why insn't the weather's widget is not working\" -- many issues\n","\n","import random\n","sample = random.sample(range(0, len(temp.summaries)), 20)\n","#print(temp.summaries.iloc[sample].to_list())\n","\n","# ----------------------------------------------------------------------------------\n","\n","# Do the summaries reflect the original conversation?\n","\n","# Thoughts:\n","# Despite what I generally believe is poor summary quality, in terms of \"readability\", the \n","# ground truth summaries do reflect the key points of the dialogues. \n","\n","for i in sample: \n","  print(f\"Conversation: {temp.inputs.iloc[i]}\")\n","  print(f\"Summary: {temp.summaries.iloc[i]}\\n\")\n","\n","# ----------------------------------------------------------------------------------"],"execution_count":null,"outputs":[]}]}